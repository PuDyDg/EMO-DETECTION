{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Cài đặt những thư viện cần thiết**"
      ],
      "metadata": {
        "id": "9AqlI_pbJzQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-community langchain-huggingface chromadb pymupdf transformers accelerate torch datasets\n",
        "!pip install --upgrade transformers"
      ],
      "metadata": {
        "id": "bnv4BnvVJVim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import json\n",
        "from huggingface_hub import login\n",
        "from datasets import load_dataset\n",
        "from transformers import Trainer, TrainingArguments\n",
        "import os\n"
      ],
      "metadata": {
        "id": "AdQNM8xGJvVr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "83ff78c5-6cd3-4e4b-ed09-72f4b186f738"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-6e6c2ee37644>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhuggingface_hub\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Check the dependencies satisfy the minimal versions required.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdependency_versions_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m from .utils import (\n\u001b[1;32m     28\u001b[0m     \u001b[0mOptionalDependencyNotAvailable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/dependency_versions_check.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdependency_versions_table\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequire_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_version_core\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbackbone_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBackboneConfigMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBackboneMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mchat_template_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDocstringParsingException\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeHintParsingException\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_json_schema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconstants\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIMAGENET_DEFAULT_MEAN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMAGENET_DEFAULT_STD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMAGENET_STANDARD_MEAN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMAGENET_STANDARD_STD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m from .doc import (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/chat_template_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2603\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTYPE_CHECKING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2604\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_meta_registrations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2606\u001b[0m \u001b[0;31m# Enable CUDA Sanitizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_meta_registrations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   3896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3897\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3898\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mregister_meta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maten\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3899\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmeta_bmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmat2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3900\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcommon_meta_baddbmm_bmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmat2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, op_name)\u001b[0m\n\u001b[1;32m   1228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1229\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1230\u001b[0;31m             \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverload_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_packet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqualified_op_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1231\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mop\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m                 raise AttributeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m_get_packet\u001b[0;34m(qualname, op_module)\u001b[0m\n\u001b[1;32m   1253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_get_packet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqualname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m     \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverload_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_get_operation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqualname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mop\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1257\u001b[0m         \u001b[0;31m# let the script frontend know that op is identical to the builtin op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Logging vào Hugging**"
      ],
      "metadata": {
        "id": "qDf0JRbvKBtI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "login(\"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\")"
      ],
      "metadata": {
        "id": "2MiaFB_sKHrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Chuyển file thành dạng json**"
      ],
      "metadata": {
        "id": "Bv7FPP7fKS63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "documents = []\n",
        "with open(\"/content/train.txt\", \"r\", encoding = \"utf-8\") as f:\n",
        "  for line in f :\n",
        "    documents.append(line)\n",
        "\n",
        "\n",
        "with open(\"/content/train.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for line in documents:\n",
        "        sentence, label = line.strip().split(\";\")  # Tách câu và nhãn cảm xúc\n",
        "        json.dump({\"text\": sentence, \"label\": label}, f)\n",
        "        f.write(\"\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "6bMbByfaKYVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Load LLM**"
      ],
      "metadata": {
        "id": "yyybReN-Pa4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_name = \"distilbert-base-uncased\"  # Hoặc \"bert-base-multilingual-cased\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=6)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "id": "_k9w-sm_SnNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Load dataset**"
      ],
      "metadata": {
        "id": "Px5buEbIS-Vn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset = load_dataset(\"json\", data_files=\"train.jsonl\")\n",
        "\n",
        "# Ánh xạ nhãn thành số\n",
        "label_mapping = {\"sadness\": 0, \"joy\": 1, \"anger\": 2, \"love\" : 3, \"surprise\" : 4, \"fear\" : 5}\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    tokens = tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "    tokens[\"label\"] = [label_mapping[label] for label in examples[\"label\"]]  # Chuyển từng nhãn thành số\n",
        "    return tokens\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "IO4CRcheS4DG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **TRAIN TIME!!!**"
      ],
      "metadata": {
        "id": "2f5zrBfFTkoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./fine_tuned_sentiment\",\n",
        "    evaluation_strategy=\"no\",  # Không cần tập validation\n",
        "    save_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=5,\n",
        "    logging_dir=\"./logs\",\n",
        "    save_total_limit=2,\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    push_to_hub=False\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model(\"./fine_tuned_sentiment\")\n"
      ],
      "metadata": {
        "id": "Ux0UFh_7Tj0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Kiểm tra**"
      ],
      "metadata": {
        "id": "bKu7NpDqP8tk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ánh xạ số về tên nhãn\n",
        "label_mapping = {0: \"sadness\", 1: \"joy\", 2: \"anger\", 3: \"love\", 4: \"surprise\", 5: \"fear\"}\n",
        "\n",
        "# Tạo pipeline\n",
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"text-classification\", model=\"./fine_tuned_sentiment\", tokenizer=tokenizer)\n",
        "\n",
        "true_answers = 0\n",
        "questions = 0\n",
        "with open(\"/content/test.txt\", \"r\", encoding = \"utf-8\") as f:\n",
        "  for line in f :\n",
        "    test_sentence, emotion = line.strip().split(';')\n",
        "    result = classifier(test_sentence)[0]  # Lấy kết quả đầu tiên\n",
        "    result[\"label\"] = label_mapping[int(result[\"label\"].split(\"_\")[-1])]\n",
        "    if result[\"label\"] == emotion : true_answers += 1\n",
        "    questions += 1\n",
        "\n",
        "print(true_answers, ' ', questions)"
      ],
      "metadata": {
        "id": "pqvm14PQUBvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ánh xạ số về tên nhãn\n",
        "label_mapping = {0: \"sadness\", 1: \"joy\", 2: \"anger\", 3: \"love\", 4: \"surprise\", 5: \"fear\"}\n",
        "\n",
        "# Tạo pipeline\n",
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"text-classification\", model=\"distilbert-base-uncased\", tokenizer=tokenizer)\n",
        "\n",
        "true_answers = 0\n",
        "questions = 0\n",
        "with open(\"/content/test.txt\", \"r\", encoding = \"utf-8\") as f:\n",
        "  for line in f :\n",
        "    test_sentence, emotion = line.strip().split(';')\n",
        "    result = classifier(test_sentence)[0]  # Lấy kết quả đầu tiên\n",
        "    result[\"label\"] = label_mapping[int(result[\"label\"].split(\"_\")[-1])]\n",
        "    if result[\"label\"] == emotion : true_answers += 1\n",
        "    questions += 1\n",
        "\n",
        "print(true_answers, ' ', questions)"
      ],
      "metadata": {
        "id": "6fwwkIyUmDr8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}